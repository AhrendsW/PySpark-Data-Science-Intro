{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6DATyCRnjp8"
   },
   "source": [
    "# Carregamento de Dados\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
    "\n",
    "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
    "\n",
    "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = r'C:\\Spark\\spark-3.1.2-bin-hadoop2.7'\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jre1.8.0_421'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9gz8XCfu-J_a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Iniciando com Spark\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "6uQuAEDs-a-2",
    "outputId": "e72c66c8-1c99-4479-f012-782477a2c6be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Iniciando com Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cdd64b4520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp-4bOYFnw0L"
   },
   "source": [
    "## DataFrames com Spark\n",
    "\n",
    "\n",
    "### Interfaces Spark\n",
    "\n",
    "Existem três interfaces principais do Apache Spark que você deve conhecer: Resilient Distributed Dataset, DataFrame e Dataset.\n",
    "\n",
    "- **Resilient Distributed Dataset**: A primeira abstração do Apache Spark foi o Resilient Distributed Dataset (RDD). É uma interface para uma sequência de objetos de dados que consiste em um ou mais tipos localizados em uma coleção de máquinas (um cluster). Os RDDs podem ser criados de várias maneiras e são a API de “nível mais baixo” disponível. Embora esta seja a estrutura de dados original do Apache Spark, você deve se concentrar na API DataFrame, que é um superconjunto da funcionalidade RDD. A API RDD está disponível nas linguagens Java, Python e Scala.\n",
    "\n",
    "- **DataFrame**: Trata-se de um conceito similar ao DataFrame que você pode estar familiarizado como o pacote pandas do Python e a linguagem R . A API DataFrame está disponível nas linguagens Java, Python, R e Scala.\n",
    "\n",
    "- **Dataset**: uma combinação de DataFrame e RDD. Ele fornece a interface digitada que está disponível em RDDs enquanto fornece a conveniência do DataFrame. A API Dataset está disponível nas linguagens Java e Scala.\n",
    "\n",
    "Em muitos cenários, especialmente com as otimizações de desempenho incorporadas em DataFrames e Datasets, não será necessário trabalhar com RDDs. Mas é importante entender a abstração RDD porque:\n",
    "\n",
    "- O RDD é a infraestrutura subjacente que permite que o Spark seja executado com tanta rapidez e forneça a linhagem de dados.\n",
    "\n",
    "- Se você estiver mergulhando em componentes mais avançados do Spark, pode ser necessário usar RDDs.\n",
    "\n",
    "- As visualizações na Spark UI fazem referência a RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FUJnXwtw-btQ"
   },
   "outputs": [],
   "source": [
    "data = [('Zeca','35'), ('Eva', '29')]\n",
    "colNames = ['Nome', 'Idade']\n",
    "df = spark.createDataFrame(data, colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Idade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ4LI7UroBRQ",
    "outputId": "f8d4e35a-9909-46c4-e065-77b578d878ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Nome|Idade|\n",
      "+----+-----+\n",
      "|Zeca|   35|\n",
      "| Eva|   29|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "gmspd22oBbtw",
    "outputId": "617a53d6-8d2b-4e0f-d19b-e12201c2cd2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome</th>\n",
       "      <th>Idade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zeca</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eva</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nome Idade\n",
       "0  Zeca    35\n",
       "1   Eva    29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto\n",
    "\n",
    "Nosso projeto consiste em ler, manipular, tratar e salvar um conjunto de dados volumosos utilizando como ferramenta o Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf0FNCJOoI3M"
   },
   "source": [
    "## Carregamento de dados\n",
    "\n",
    "### Dados Públicos CNPJ\n",
    "#### Receita Federal\n",
    "\n",
    "> [Empresas](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/empresas.zip)\n",
    ">\n",
    "> [Estabelecimentos](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/estabelecimentos.zip)\n",
    ">\n",
    "> [Sócios](https://caelum-online-public.s3.amazonaws.com/2273-introducao-spark/01/socios.zip)\n",
    "\n",
    "[Fonte original dos dados](https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj)\n",
    "\n",
    "---\n",
    "[property SparkSession.read](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html)\n",
    "\n",
    "[DataFrameReader.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsKh3VDDo0Ok"
   },
   "source": [
    "### Carregando os dados das empresas, estabelecimentos e socios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "weTkFja6Dam2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "pathDefault = r'H:\\Projetos\\PySpark-Data-Science-Intro\\data'\n",
    "empresasZip, estabelecimentosZip, sociosZip = 'empresas.zip', 'estabelecimentos.zip', 'socios.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmQWnzxPEy-1",
    "outputId": "5c55c72a-bbfb-44c5-fe5a-3132a038d83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao descompactar as pastas, verifique o caminho e tente novamente.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  zipfile.ZipFile(os.path.join(pathDefault, empresasZip), 'r').extractall(pathDefault)\n",
    "  zipfile.ZipFile(os.path.join(pathDefault, estabelecimentosZip), 'r').extractall(pathDefault)\n",
    "  zipfile.ZipFile(os.path.join(pathDefault, sociosZip), 'r').extractall(pathDefault)\n",
    "except Exception as e:\n",
    "  print('Erro ao descompactar as pastas, verifique o caminho e tente novamente.')\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas_files = [os.path.join(pathDefault, 'empresas', f) for f in os.listdir(os.path.join(pathDefault, 'empresas')) if f.endswith('.csv')]\n",
    "socios_files = [os.path.join(pathDefault, 'socios', f) for f in os.listdir(os.path.join(pathDefault, 'socios')) if f.endswith('.csv')]\n",
    "estabelecimentos_files = [os.path.join(pathDefault, 'estabelecimentos', f) for f in os.listdir(os.path.join(pathDefault, 'estabelecimentos')) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8ZiPrx1CFGP-"
   },
   "outputs": [],
   "source": [
    "empresas = spark.read.csv(empresas_files, sep=';', inferSchema=True, header=False)\n",
    "socios = spark.read.csv(socios_files, sep=';', inferSchema=True, header=False)\n",
    "estabelecimentos = spark.read.csv(estabelecimentos_files, sep=';', inferSchema=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_wCH-ifwr-O"
   },
   "source": [
    "# Manipulando os Dados\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1H4VBfqwndM"
   },
   "source": [
    "## Operações básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jAg1JZJq_tW",
    "outputId": "70c46968-6c4f-46b2-e7f3-ee6d0dabeeea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Empresas:\n",
      "+----+--------------------------------------------------------------------------------------------+----+---+-------+---+----+\n",
      "|_c0 |_c1                                                                                         |_c2 |_c3|_c4    |_c5|_c6 |\n",
      "+----+--------------------------------------------------------------------------------------------+----+---+-------+---+----+\n",
      "|306 |FRANCAMAR REFRIGERACAO TECNICA S/C LTDA                                                     |2240|49 |0,00   |1  |null|\n",
      "|1355|BRASILEIRO & OLIVEIRA LTDA                                                                  |2062|49 |0,00   |5  |null|\n",
      "|4820|REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E TABELIONATO E REGISTRO DE CONSTRATOS MARITIMOS|3034|32 |0,00   |5  |null|\n",
      "|5347|ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS                                                |2135|50 |0,00   |5  |null|\n",
      "|6846|BADU E FILHOS TECIDOS LTDA                                                                  |2062|49 |4000,00|1  |null|\n",
      "+----+--------------------------------------------------------------------------------------------+----+---+-------+---+----+\n",
      "\n",
      "DataFrame de Estabelecimentos:\n",
      "+----+---+---+---+-----------------+---+--------+---+----+----+--------+-------+----+----+----------------------+----+-------+------------------+-------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0 |_c1|_c2|_c3|_c4              |_c5|_c6     |_c7|_c8 |_c9 |_c10    |_c11   |_c12|_c13|_c14                  |_c15|_c16   |_c17              |_c18   |_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|\n",
      "+----+---+---+---+-----------------+---+--------+---+----+----+--------+-------+----+----+----------------------+----+-------+------------------+-------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|1879|1  |96 |1  |PIRAMIDE M. C.   |8  |20011029|1  |null|null|19940509|1412602|null|RUA |JOSE FIGLIOLINI       |608 |null   |VILA NILO         |2278020|SP  |7107|null|null|null|null|null|null|null|null|null|\n",
      "|2818|1  |43 |1  |null             |8  |20081231|71 |null|null|19940512|4671100|null|RUA |BAQUIA                |416 |null   |VL NOVA MANCHESTER|3443000|SP  |7107|null|null|null|null|null|null|null|null|null|\n",
      "|3110|1  |7  |1  |null             |8  |19971231|1  |null|null|19940512|4789007|null|RUA |LEOCADIA CINTRA       |180 |CONJ 83|MOOCA             |3112040|SP  |7107|null|null|null|null|null|null|null|null|null|\n",
      "|3733|1  |80 |1  |null             |8  |20081231|71 |null|null|19940513|7490101|null|RUA |AFONSO CELSO          |1102|CASA 1 |VILA MARIANA      |4119061|SP  |7107|null|null|null|null|null|null|null|null|null|\n",
      "|4628|3  |27 |2  |EMBROIDERY & GIFT|8  |19980429|1  |null|null|19950509|4755501|null|RUA |DOUTOR GABRIEL NICOLAU|177 |null   |RUDGE RAMOS       |9632040|SP  |7075|null|null|null|null|null|null|null|null|null|\n",
      "+----+---+---+---+-----------------+---+--------+---+----+----+--------+-------+----+----+----------------------+----+-------+------------------+-------+----+----+----+----+----+----+----+----+----+----+----+\n",
      "\n",
      "DataFrame de Sócios:\n",
      "+-----+---+-------------------------------+-----------+---+--------+----+-----------+----+---+----+\n",
      "|_c0  |_c1|_c2                            |_c3        |_c4|_c5     |_c6 |_c7        |_c8 |_c9|_c10|\n",
      "+-----+---+-------------------------------+-----------+---+--------+----+-----------+----+---+----+\n",
      "|411  |2  |LILIANA PATRICIA GUASTAVINO    |***678188**|22 |19940725|null|***000000**|null|0  |7   |\n",
      "|411  |2  |CRISTINA HUNDERTMARK           |***637848**|28 |19940725|null|***000000**|null|0  |7   |\n",
      "|5813 |2  |CELSO EDUARDO DE CASTRO STEPHAN|***786068**|49 |19940516|null|***000000**|null|0  |8   |\n",
      "|5813 |2  |EDUARDO BERRINGER STEPHAN      |***442348**|49 |19940516|null|***000000**|null|0  |5   |\n",
      "|14798|2  |HANNE MAHFOUD FADEL            |***760388**|49 |19940609|null|***000000**|null|0  |8   |\n",
      "+-----+---+-------------------------------+-----------+---+--------+----+-----------+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('DataFrame de Empresas:')\n",
    "empresas.limit(5).show(truncate=False)\n",
    "\n",
    "print('DataFrame de Estabelecimentos:')\n",
    "estabelecimentos.limit(5).show(truncate=False)\n",
    "\n",
    "print('DataFrame de Sócios:')\n",
    "socios.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtZn-odWst81"
   },
   "source": [
    "### Renomeando as colunas do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yac9AKCfmJ5Z"
   },
   "outputs": [],
   "source": [
    "empresasColNames = ['cnpj_basico', 'razao_social_nome_empresarial', 'natureza_juridica', 'qualificacao_do_responsavel', 'capital_social_da_empresa', 'porte_da_empresa',\n",
    "                    'ente_federativo_responsavel']\n",
    "\n",
    "estabsColNames = ['cnpj_basico', 'cnpj_ordem', 'cnpj_dv', 'identificador_matriz_filial', 'nome_fantasia', 'situacao_cadastral', 'data_situacao_cadastral',\n",
    "                  'motivo_situacao_cadastral', 'nome_da_cidade_no_exterior', 'pais', 'data_de_inicio_atividade', 'cnae_fiscal_principal', 'cnae_fiscal_secundaria',\n",
    "                  'tipo_de_logradouro', 'logradouro', 'numero', 'complemento', 'bairro', 'cep', 'uf', 'municipio', 'ddd_1', 'telefone_1', 'ddd_2', 'telefone_2', 'ddd_do_fax',\n",
    "                  'fax', 'correio_eletronico', 'situacao_especial', 'data_da_situacao_especial']\n",
    "\n",
    "sociosColNames = ['cnpj_basico', 'identificador_de_socio', 'nome_do_socio_ou_razao_social', 'cnpj_ou_cpf_do_socio', 'qualificacao_do_socio', 'data_de_entrada_sociedade', 'pais',\n",
    "                  'representante_legal', 'nome_do_representante', 'qualificacao_do_representante_legal', 'faixa_etaria']\n",
    "\n",
    "dataframes = [empresas, estabelecimentos, socios]\n",
    "colunas = [empresasColNames, estabsColNames, sociosColNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V0Sr3QybnRAP"
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    df = dataframes[i]\n",
    "    colNames = colunas[i]\n",
    "    for index, colName in enumerate(colNames):\n",
    "        df = df.withColumnRenamed(f\"_c{index}\", colName)\n",
    "    dataframes[i] = df\n",
    "\n",
    "empresas, estabelecimentos, socios = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rViFNHNEtjRa",
    "outputId": "462ec15d-6071-4f2c-80c5-69ebf6410e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Empresas:\n",
      "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial                                                               |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
      "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|306        |FRANCAMAR REFRIGERACAO TECNICA S/C LTDA                                                     |2240             |49                         |0,00                     |1               |null                       |\n",
      "|1355       |BRASILEIRO & OLIVEIRA LTDA                                                                  |2062             |49                         |0,00                     |5               |null                       |\n",
      "|4820       |REGISTRO DE IMOVEIS, TABELIONATO 1 DE NOTAS E TABELIONATO E REGISTRO DE CONSTRATOS MARITIMOS|3034             |32                         |0,00                     |5               |null                       |\n",
      "|5347       |ROSELY APARECIDA MONTEIRO CALTABIANO FREITAS                                                |2135             |50                         |0,00                     |5               |null                       |\n",
      "|6846       |BADU E FILHOS TECIDOS LTDA                                                                  |2062             |49                         |4000,00                  |1               |null                       |\n",
      "+-----------+--------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "\n",
      "Esquema de Empresas:\n",
      "root\n",
      " |-- cnpj_basico: integer (nullable = true)\n",
      " |-- razao_social_nome_empresarial: string (nullable = true)\n",
      " |-- natureza_juridica: integer (nullable = true)\n",
      " |-- qualificacao_do_responsavel: integer (nullable = true)\n",
      " |-- capital_social_da_empresa: string (nullable = true)\n",
      " |-- porte_da_empresa: integer (nullable = true)\n",
      " |-- ente_federativo_responsavel: string (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DataFrame de Estabelecimentos:\n",
      "+-----------+----------+-------+---------------------------+-----------------+------------------+-----------------------+-------------------------+--------------------------+----+------------------------+---------------------+----------------------+------------------+----------------------+------+-----------+------------------+-------+---+---------+-----+----------+-----+----------+----------+----+------------------+-----------------+-------------------------+\n",
      "|cnpj_basico|cnpj_ordem|cnpj_dv|identificador_matriz_filial|nome_fantasia    |situacao_cadastral|data_situacao_cadastral|motivo_situacao_cadastral|nome_da_cidade_no_exterior|pais|data_de_inicio_atividade|cnae_fiscal_principal|cnae_fiscal_secundaria|tipo_de_logradouro|logradouro            |numero|complemento|bairro            |cep    |uf |municipio|ddd_1|telefone_1|ddd_2|telefone_2|ddd_do_fax|fax |correio_eletronico|situacao_especial|data_da_situacao_especial|\n",
      "+-----------+----------+-------+---------------------------+-----------------+------------------+-----------------------+-------------------------+--------------------------+----+------------------------+---------------------+----------------------+------------------+----------------------+------+-----------+------------------+-------+---+---------+-----+----------+-----+----------+----------+----+------------------+-----------------+-------------------------+\n",
      "|1879       |1         |96     |1                          |PIRAMIDE M. C.   |8                 |20011029               |1                        |null                      |null|19940509                |1412602              |null                  |RUA               |JOSE FIGLIOLINI       |608   |null       |VILA NILO         |2278020|SP |7107     |null |null      |null |null      |null      |null|null              |null             |null                     |\n",
      "|2818       |1         |43     |1                          |null             |8                 |20081231               |71                       |null                      |null|19940512                |4671100              |null                  |RUA               |BAQUIA                |416   |null       |VL NOVA MANCHESTER|3443000|SP |7107     |null |null      |null |null      |null      |null|null              |null             |null                     |\n",
      "|3110       |1         |7      |1                          |null             |8                 |19971231               |1                        |null                      |null|19940512                |4789007              |null                  |RUA               |LEOCADIA CINTRA       |180   |CONJ 83    |MOOCA             |3112040|SP |7107     |null |null      |null |null      |null      |null|null              |null             |null                     |\n",
      "|3733       |1         |80     |1                          |null             |8                 |20081231               |71                       |null                      |null|19940513                |7490101              |null                  |RUA               |AFONSO CELSO          |1102  |CASA 1     |VILA MARIANA      |4119061|SP |7107     |null |null      |null |null      |null      |null|null              |null             |null                     |\n",
      "|4628       |3         |27     |2                          |EMBROIDERY & GIFT|8                 |19980429               |1                        |null                      |null|19950509                |4755501              |null                  |RUA               |DOUTOR GABRIEL NICOLAU|177   |null       |RUDGE RAMOS       |9632040|SP |7075     |null |null      |null |null      |null      |null|null              |null             |null                     |\n",
      "+-----------+----------+-------+---------------------------+-----------------+------------------+-----------------------+-------------------------+--------------------------+----+------------------------+---------------------+----------------------+------------------+----------------------+------+-----------+------------------+-------+---+---------+-----+----------+-----+----------+----------+----+------------------+-----------------+-------------------------+\n",
      "\n",
      "Esquema de Estabelecimentos:\n",
      "root\n",
      " |-- cnpj_basico: integer (nullable = true)\n",
      " |-- cnpj_ordem: integer (nullable = true)\n",
      " |-- cnpj_dv: integer (nullable = true)\n",
      " |-- identificador_matriz_filial: integer (nullable = true)\n",
      " |-- nome_fantasia: string (nullable = true)\n",
      " |-- situacao_cadastral: integer (nullable = true)\n",
      " |-- data_situacao_cadastral: integer (nullable = true)\n",
      " |-- motivo_situacao_cadastral: integer (nullable = true)\n",
      " |-- nome_da_cidade_no_exterior: string (nullable = true)\n",
      " |-- pais: integer (nullable = true)\n",
      " |-- data_de_inicio_atividade: integer (nullable = true)\n",
      " |-- cnae_fiscal_principal: integer (nullable = true)\n",
      " |-- cnae_fiscal_secundaria: string (nullable = true)\n",
      " |-- tipo_de_logradouro: string (nullable = true)\n",
      " |-- logradouro: string (nullable = true)\n",
      " |-- numero: string (nullable = true)\n",
      " |-- complemento: string (nullable = true)\n",
      " |-- bairro: string (nullable = true)\n",
      " |-- cep: integer (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- municipio: integer (nullable = true)\n",
      " |-- ddd_1: string (nullable = true)\n",
      " |-- telefone_1: string (nullable = true)\n",
      " |-- ddd_2: string (nullable = true)\n",
      " |-- telefone_2: string (nullable = true)\n",
      " |-- ddd_do_fax: integer (nullable = true)\n",
      " |-- fax: string (nullable = true)\n",
      " |-- correio_eletronico: string (nullable = true)\n",
      " |-- situacao_especial: string (nullable = true)\n",
      " |-- data_da_situacao_especial: integer (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DataFrame de Sócios:\n",
      "+-----------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "|cnpj_basico|identificador_de_socio|nome_do_socio_ou_razao_social  |cnpj_ou_cpf_do_socio|qualificacao_do_socio|data_de_entrada_sociedade|pais|representante_legal|nome_do_representante|qualificacao_do_representante_legal|faixa_etaria|\n",
      "+-----------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "|411        |2                     |LILIANA PATRICIA GUASTAVINO    |***678188**         |22                   |19940725                 |null|***000000**        |null                 |0                                  |7           |\n",
      "|411        |2                     |CRISTINA HUNDERTMARK           |***637848**         |28                   |19940725                 |null|***000000**        |null                 |0                                  |7           |\n",
      "|5813       |2                     |CELSO EDUARDO DE CASTRO STEPHAN|***786068**         |49                   |19940516                 |null|***000000**        |null                 |0                                  |8           |\n",
      "|5813       |2                     |EDUARDO BERRINGER STEPHAN      |***442348**         |49                   |19940516                 |null|***000000**        |null                 |0                                  |5           |\n",
      "|14798      |2                     |HANNE MAHFOUD FADEL            |***760388**         |49                   |19940609                 |null|***000000**        |null                 |0                                  |8           |\n",
      "+-----------+----------------------+-------------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "\n",
      "Esquema de Sócios:\n",
      "root\n",
      " |-- cnpj_basico: integer (nullable = true)\n",
      " |-- identificador_de_socio: integer (nullable = true)\n",
      " |-- nome_do_socio_ou_razao_social: string (nullable = true)\n",
      " |-- cnpj_ou_cpf_do_socio: string (nullable = true)\n",
      " |-- qualificacao_do_socio: integer (nullable = true)\n",
      " |-- data_de_entrada_sociedade: integer (nullable = true)\n",
      " |-- pais: integer (nullable = true)\n",
      " |-- representante_legal: string (nullable = true)\n",
      " |-- nome_do_representante: string (nullable = true)\n",
      " |-- qualificacao_do_representante_legal: integer (nullable = true)\n",
      " |-- faixa_etaria: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('DataFrame de Empresas:')\n",
    "empresas.limit(5).show(truncate=False)\n",
    "print('Esquema de Empresas:')\n",
    "empresas.printSchema()\n",
    "print('----' * 20)\n",
    "\n",
    "print('DataFrame de Estabelecimentos:')\n",
    "estabelecimentos.limit(5).show(truncate=False)\n",
    "print('Esquema de Estabelecimentos:')\n",
    "estabelecimentos.printSchema()\n",
    "print('----' * 20)\n",
    "\n",
    "print('DataFrame de Sócios:')\n",
    "socios.limit(5).show(truncate=False)\n",
    "print('Esquema de Sócios:')\n",
    "socios.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9i0KBK-w_TJ"
   },
   "source": [
    "### Convertendo String ➔ Double\n",
    "\n",
    "#### `StringType ➔ DoubleType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_s8m0AJCpJVS"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "roRI-nbZqJOc"
   },
   "outputs": [],
   "source": [
    "empresas = empresas.withColumn('capital_social_da_empresa', f.regexp_replace('capital_social_da_empresa', ',', '.').cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JykEluPGxhzN"
   },
   "source": [
    "### Convertendo String ➔ Date\n",
    "\n",
    "#### `StringType ➔ DateType`\n",
    "\n",
    "[Datetime Patterns](https://spark.apache.org/docs/3.1.2/sql-ref-datetime-pattern.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bs5q9DGcq1PK"
   },
   "outputs": [],
   "source": [
    "socios = socios\\\n",
    "      .withColumn(\n",
    "          'data_de_entrada_sociedade',\n",
    "          f.to_date(f.col('data_de_entrada_sociedade').cast(StringType()), 'yyyyMMdd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XXYh3R3qv9rI"
   },
   "outputs": [],
   "source": [
    "estabelecimentos = estabelecimentos\\\n",
    "    .withColumn(\n",
    "        'data_situacao_cadastral',\n",
    "        f.to_date(f.col('data_situacao_cadastral').cast(StringType()), 'yyyyMMdd')\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        'data_de_inicio_atividade',\n",
    "        f.to_date(f.col('data_de_inicio_atividade').cast(StringType()), 'yyyyMMdd')\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        'data_da_situacao_especial',\n",
    "        f.to_date(f.col('data_da_situacao_especial').cast(StringType()), 'yyyyMMdd')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-5D3Hkbx1WR"
   },
   "source": [
    "# Seleções e consultas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGkyoEhcx4hc"
   },
   "source": [
    "## Selecionando e Ordenando informações\n",
    "\n",
    "[DataFrame.select(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.select.html)\n",
    "\n",
    "[DataFrame.orderBy(*cols, **kwargs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEaPLZDIXMCh",
    "outputId": "60f445ee-7cfc-4544-80d9-0434dc94651d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------+------------+\n",
      "|nome_do_socio_ou_razao_social  |faixa_etaria|AnoDeEntrada|\n",
      "+-------------------------------+------------+------------+\n",
      "|JOAO FRANCISCO DE AMORIM JUNCAL|3           |null        |\n",
      "|MARIA SILENE BEZERRA DE AGUIAR |8           |1900        |\n",
      "|NAIR YOKO HIRAI TAKAKI         |7           |1900        |\n",
      "|JOSE NELSON VIEIRA CAMPOS      |6           |1901        |\n",
      "|VALMAR CARDOSO DE SANTANA      |5           |1901        |\n",
      "+-------------------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "socios\\\n",
    "      .select('nome_do_socio_ou_razao_social', 'faixa_etaria', f.year('data_de_entrada_sociedade').alias('AnoDeEntrada'))\\\n",
    "      .orderBy('AnoDeEntrada')\\\n",
    "      .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fy0rWmP6Xipa",
    "outputId": "3f370caa-2c35-4636-c95d-966eb56d61d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+---------+--------------------+-------------------+\n",
      "|nome_fantasia                               |municipio|MesDeInicioAtividade|AnoDeIncioAtividade|\n",
      "+--------------------------------------------+---------+--------------------+-------------------+\n",
      "|NEW DISTRIBUIDORA                           |301      |5                   |2021               |\n",
      "|AM SUL REPRESENTECAO                        |8589     |5                   |2021               |\n",
      "|ANDERSON BARROSO                            |2631     |5                   |2021               |\n",
      "|DANKEL DISTRIBUIDORA DE BEBIDAS             |8985     |5                   |2021               |\n",
      "|BIANCA TERAPEUTA                            |1389     |5                   |2021               |\n",
      "|OTTIMO PUNTO                                |7535     |5                   |2021               |\n",
      "|ARILSON ALESSANDRO LF DANCE E FITNESS       |6001     |5                   |2021               |\n",
      "|TZ TIOZAOANDABAIXO                          |7077     |5                   |2021               |\n",
      "|MARCELO DE JESUS NUNES                      |8327     |5                   |2021               |\n",
      "|OUTLET DAS UNHAS 25                         |7107     |5                   |2021               |\n",
      "|Nome Fantasia não encontrado                |4521     |5                   |2021               |\n",
      "|GT DISTRIBUIDORA                            |8655     |5                   |2021               |\n",
      "|Nome Fantasia não encontrado                |9093     |5                   |2021               |\n",
      "|GERSON FERNANDES DESENVOLVIMENTO DE SOFTWARE|3849     |5                   |2021               |\n",
      "|RICARDO BUENO                               |8801     |5                   |2021               |\n",
      "+--------------------------------------------+---------+--------------------+-------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estabelecimentos\\\n",
    "      .select('nome_fantasia', 'municipio', f.month('data_de_inicio_atividade').alias('MesDeInicioAtividade'), f.year('data_de_inicio_atividade').alias('AnoDeIncioAtividade'))\\\n",
    "      .na.fill('Nome Fantasia não encontrado')\\\n",
    "      .orderBy(['AnoDeIncioAtividade', 'MesDeInicioAtividade'], ascending=[False, False])\\\n",
    "      .show(15, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt7DdH4LyU2S"
   },
   "source": [
    "## Identificando valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsaKdhtAaWKu",
    "outputId": "5cc89aab-15a8-44c2-d8c4-223f50af0c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de valores nulos por coluna:\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+-------+-------------------+---------------------+-----------------------------------+------------+\n",
      "|cnpj_basico|identificador_de_socio|nome_do_socio_ou_razao_social|cnpj_ou_cpf_do_socio|qualificacao_do_socio|data_de_entrada_sociedade|   pais|representante_legal|nome_do_representante|qualificacao_do_representante_legal|faixa_etaria|\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+-------+-------------------+---------------------+-----------------------------------+------------+\n",
      "|          0|                     0|                          208|                1234|                    0|                        1|2038255|                  0|              1995432|                                  0|           0|\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+-------+-------------------+---------------------+-----------------------------------+------------+\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Substituindo valores nulos não string por 0:\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "|cnpj_basico|identificador_de_socio|nome_do_socio_ou_razao_social|cnpj_ou_cpf_do_socio|qualificacao_do_socio|data_de_entrada_sociedade|pais|representante_legal|nome_do_representante|qualificacao_do_representante_legal|faixa_etaria|\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "|        411|                     2|         LILIANA PATRICIA ...|         ***678188**|                   22|               1994-07-25|   0|        ***000000**|                 null|                                  0|           7|\n",
      "|        411|                     2|         CRISTINA HUNDERTMARK|         ***637848**|                   28|               1994-07-25|   0|        ***000000**|                 null|                                  0|           7|\n",
      "|       5813|                     2|         CELSO EDUARDO DE ...|         ***786068**|                   49|               1994-05-16|   0|        ***000000**|                 null|                                  0|           8|\n",
      "|       5813|                     2|         EDUARDO BERRINGER...|         ***442348**|                   49|               1994-05-16|   0|        ***000000**|                 null|                                  0|           5|\n",
      "|      14798|                     2|          HANNE MAHFOUD FADEL|         ***760388**|                   49|               1994-06-09|   0|        ***000000**|                 null|                                  0|           8|\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Substituindo valores nulos do tipo string por -:\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "|cnpj_basico|identificador_de_socio|nome_do_socio_ou_razao_social|cnpj_ou_cpf_do_socio|qualificacao_do_socio|data_de_entrada_sociedade|pais|representante_legal|nome_do_representante|qualificacao_do_representante_legal|faixa_etaria|\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "|        411|                     2|         LILIANA PATRICIA ...|         ***678188**|                   22|               1994-07-25|null|        ***000000**|                    -|                                  0|           7|\n",
      "|        411|                     2|         CRISTINA HUNDERTMARK|         ***637848**|                   28|               1994-07-25|null|        ***000000**|                    -|                                  0|           7|\n",
      "|       5813|                     2|         CELSO EDUARDO DE ...|         ***786068**|                   49|               1994-05-16|null|        ***000000**|                    -|                                  0|           8|\n",
      "|       5813|                     2|         EDUARDO BERRINGER...|         ***442348**|                   49|               1994-05-16|null|        ***000000**|                    -|                                  0|           5|\n",
      "|      14798|                     2|          HANNE MAHFOUD FADEL|         ***760388**|                   49|               1994-06-09|null|        ***000000**|                    -|                                  0|           8|\n",
      "+-----------+----------------------+-----------------------------+--------------------+---------------------+-------------------------+----+-------------------+---------------------+-----------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de valores nulos por coluna:')\n",
    "socios.select([f.count(f.when(f.isnull(c), 1)).alias(c) for c in socios.columns]).show(5)\n",
    "print('----' * 20)\n",
    "print('Substituindo valores nulos não string por 0:')\n",
    "socios.na.fill(0).limit(5).show(5)\n",
    "print('----' * 20)\n",
    "print('Substituindo valores nulos do tipo string por -:')\n",
    "socios.na.fill('-').limit(5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCZ_GX8JfjGG"
   },
   "source": [
    "## Filtrando os dados\n",
    "\n",
    "[DataFrame.where(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.where.html) ou [DataFrame.filter(condition)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.filter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdyhDIpUfl9H",
    "outputId": "49a40f1f-726d-4986-8104-72cd4c4c50da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial       |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
      "+-----------+------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|17350147   |ERIK MARCELO DOS SANTOS 42107848858 |2135             |50                         |50.0                     |1               |null                       |\n",
      "|17833214   |ALEXANDRE MACHADO LIMA 73750123772  |2135             |50                         |50.0                     |1               |null                       |\n",
      "|20860830   |YASMIN MOURA DA FONSECA 13457709793 |2135             |50                         |50.0                     |1               |null                       |\n",
      "|22242856   |JOAO CESAR MESSIAS 08707149883      |2135             |50                         |50.0                     |1               |null                       |\n",
      "|23238540   |EVERTON ROBERTO DA SILVA 42101963809|2135             |50                         |50.0                     |1               |null                       |\n",
      "+-----------+------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empresas\\\n",
    "      .where(f.col('capital_social_da_empresa') == 50)\\\n",
    "      .limit(5).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geS8eylKgUan",
    "outputId": "67ca5673-2698-4bc7-da1c-82cf2f758710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-------------------------+\n",
      "|nome_do_socio_ou_razao_social          |data_de_entrada_sociedade|\n",
      "+---------------------------------------+-------------------------+\n",
      "|GABRIEL LAMBORGHINI ZANDOMENICO        |2002-01-16               |\n",
      "|GABRIEL CHAGAS NETO                    |2002-10-03               |\n",
      "|GABRIELLA PACHECO DE ABREU GRANDE POUSA|2002-01-07               |\n",
      "|GABRIELA LEITE BRANDAO                 |2002-06-21               |\n",
      "|GABRIELA LARA DE CASTRO SIMAO          |2002-07-17               |\n",
      "|GABRIEL MOREIRA FRANK                  |2002-03-27               |\n",
      "|GABRIELA INES CANALE MARTINS           |2002-04-12               |\n",
      "|GABRIELA MAURELL FERREIRA              |2002-07-31               |\n",
      "|GABRIELLE FERREIRA ARIOSA              |2002-06-07               |\n",
      "|GABRIELA CAXAMBU DOS SANTOS            |2002-09-09               |\n",
      "+---------------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "socios\\\n",
    "    .select(\"nome_do_socio_ou_razao_social\", \"data_de_entrada_sociedade\")\\\n",
    "    .filter(f.year('data_de_entrada_sociedade') >= 2002)\\\n",
    "    .filter(f.col('nome_do_socio_ou_razao_social').startswith(\"GABRIEL\"))\\\n",
    "    .orderBy(f.year('data_de_entrada_sociedade').asc())\\\n",
    "    .limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E45OFKjkjrc2"
   },
   "source": [
    "## O comando LIKE\n",
    "\n",
    "[Column.like(other)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.Column.like.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kw82yuktj40T",
    "outputId": "9cd5d00c-665b-40b6-b226-f09a64500f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial                                        |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
      "+-----------+---------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|21422233   |ASSOCIACAO ATLETICA BANCO DO BRASIL                                  |3999             |16                         |0.0                      |5               |null                       |\n",
      "|18174037   |ASSOCIACAO ATLETICA BANCO DO BRASIL                                  |3999             |16                         |0.0                      |5               |null                       |\n",
      "|11200151   |ASSOCIACAO ATLETICA BANCO DO BRASIL                                  |3999             |16                         |0.0                      |5               |null                       |\n",
      "|77297513   |ASSOCIACAO ATLETICA BANCO DO BRASIL                                  |3999             |16                         |0.0                      |5               |null                       |\n",
      "|2425438    |ASSOCIACAO BENEFICIENTE DE FUNCIONARIOS DO BANCO DO BRASILEM JACOBINA|3999             |16                         |0.0                      |5               |null                       |\n",
      "+-----------+---------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empresas\\\n",
    "    .where(f.upper('razao_social_nome_empresarial').like('%BANCO DO BRASIL%'))\\\n",
    "    .limit(5).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCPQWC1qkAWi",
    "outputId": "d947e783-493f-47ee-d8a0-532a39509d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial                                                                                               |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
      "+-----------+----------------------------------------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|13401188   |BRADESCO FUNDO DE INVESTIMENTO EM COTAS DE FUNDO DE INVESTIMENTO RENDA FIXA TITULOS DO TESOURO                              |2224             |43                         |0.0                      |5               |null                       |\n",
      "|38330325   |BRADESCO FUNDO DE INVESTIMENTO RENDA FIXA - 1415                                                                            |2224             |43                         |0.0                      |5               |null                       |\n",
      "|3546550    |BRADESCO - FUNDO DE INVESTIMENTO FINANCEIRO - SPECIAL                                                                       |2224             |43                         |0.0                      |5               |null                       |\n",
      "|6865916    |BRADESCO PRIME H FUNDO DE INVESTIMENTO EM COTAS DE FUNDOS DE INVESTIMENTO RENDA FIXA CREDITO PRIVADO LONGO PRAZO PERFORMANCE|2224             |43                         |0.0                      |5               |null                       |\n",
      "|17488659   |BRADESCO FUNDO DE INVESTIMENTO EM ACOES MASTER IBRX                                                                         |2224             |43                         |0.0                      |5               |null                       |\n",
      "+-----------+----------------------------------------------------------------------------------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empresas\\\n",
    "    .where(f.upper('razao_social_nome_empresarial').like('BRADESCO%'))\\\n",
    "    .limit(5).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7jPxmplkqvR",
    "outputId": "b2038b8d-81c5-4a27-bda9-3678904bda63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial                   |natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
      "+-----------+------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|60865763   |ITAU GRAFICA LTDA GRUPO ITAU                    |2062             |49                         |0.0                      |5               |null                       |\n",
      "|916136     |ASSOCIACAO DOS PRODUTORES DE CAJU DE ITAU       |3999             |16                         |0.0                      |5               |null                       |\n",
      "|45593704   |ASSOCIACAO DOS EX FUNCIONARIOS DA CIMENTO ITAU  |3999             |16                         |0.0                      |5               |null                       |\n",
      "|7503222    |ASSOCIACAO DE DESENVOLVIMENTO DOS JOVENS DE ITAU|3999             |16                         |0.0                      |5               |null                       |\n",
      "|42267898   |ASSOCIACAO CUBO COWORKING ITAU                  |3999             |10                         |0.0                      |5               |null                       |\n",
      "+-----------+------------------------------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empresas\\\n",
    "    .where(f.upper('razao_social_nome_empresarial').like('%ITAU'))\\\n",
    "    .limit(5).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybuYrUoel6ek"
   },
   "source": [
    "# Agregações e Junções\n",
    "---\n",
    "\n",
    "[DataFrame.groupBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)\n",
    "\n",
    "[DataFrame.agg(*exprs)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\n",
    "\n",
    "[DataFrame.summary(*statistics)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html)\n",
    "\n",
    "> Funções:\n",
    "[approx_count_distinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html) |\n",
    "[avg](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.avg.html) |\n",
    "[collect_list](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_list.html) |\n",
    "[collect_set](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.collect_set.html) |\n",
    "[countDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.countDistinct.html) |\n",
    "[count](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.count.html) |\n",
    "[grouping](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.grouping.html) |\n",
    "[first](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.first.html) |\n",
    "[last](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.last.html) |\n",
    "[kurtosis](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.kurtosis.html) |\n",
    "[max](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.max.html) |\n",
    "[min](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.min.html) |\n",
    "[mean](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.mean.html) |\n",
    "[skewness](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.skewness.html) |\n",
    "[stddev ou stddev_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev.html) |\n",
    "[stddev_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.stddev_pop.html) |\n",
    "[sum](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sum.html) |\n",
    "[sumDistinct](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.sumDistinct.html) |\n",
    "[variance ou var_samp](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.variance.html) |\n",
    "[var_pop](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.var_pop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQXhGOYfm-Y9",
    "outputId": "40281e36-1c6d-4fc2-bad9-8edec1635e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|AnoDeEntrada|count |\n",
      "+------------+------+\n",
      "|2010        |79337 |\n",
      "|2011        |83906 |\n",
      "|2012        |80101 |\n",
      "|2013        |83919 |\n",
      "|2014        |80590 |\n",
      "|2015        |80906 |\n",
      "|2016        |81587 |\n",
      "|2017        |90221 |\n",
      "|2018        |99935 |\n",
      "|2019        |118248|\n",
      "|2020        |125927|\n",
      "|2021        |56316 |\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "socios\\\n",
    "    .select(f.year('data_de_entrada_sociedade').alias('AnoDeEntrada'))\\\n",
    "    .where(f.year('data_de_entrada_sociedade') >= 2010)\\\n",
    "    .groupBy('AnoDeEntrada')\\\n",
    "    .count()\\\n",
    "    .orderBy('AnoDeEntrada', ascending=True)\\\n",
    "    .limit(15).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C53XwEyEzwcG",
    "outputId": "2d49c8cd-ba67-4ee8-9c51-6c9f39fd91ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------------+\n",
      "|porte_da_empresa|MédiaCapitalSocial|QuantidadeEmpresas|\n",
      "+----------------+------------------+------------------+\n",
      "|3               |2601001.7677092687|115151            |\n",
      "|5               |708660.4208249793 |1335500           |\n",
      "|1               |339994.53313507047|3129043           |\n",
      "|null            |8.35421888053467  |5985              |\n",
      "+----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empresas\\\n",
    "    .select('cnpj_basico', 'porte_da_empresa', 'capital_social_da_empresa')\\\n",
    "    .groupBy('porte_da_empresa')\\\n",
    "    .agg(\n",
    "        f.avg('capital_social_da_empresa').alias('MédiaCapitalSocial'),\n",
    "        f.count('cnpj_basico').alias('QuantidadeEmpresas')\n",
    "    )\\\n",
    "    .orderBy('MédiaCapitalSocial', ascending=False)\\\n",
    "    .limit(15).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxdjnBKY21Hp",
    "outputId": "4c6dcb4e-7259-45f3-d692-8fb60a63c25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+\n",
      "|summary|capital_social_da_empresa|\n",
      "+-------+-------------------------+\n",
      "|  count|                  4585679|\n",
      "|   mean|        503694.5478542674|\n",
      "| stddev|     2.1118691490537727E8|\n",
      "|    min|                      0.0|\n",
      "|    25%|                      0.0|\n",
      "|    50%|                   1000.0|\n",
      "|    75%|                   7000.0|\n",
      "|    max|         3.22014670262E11|\n",
      "+-------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empresas\\\n",
    "    .select('capital_social_da_empresa')\\\n",
    "    .summary()\\\n",
    "    .show()\n",
    "\n",
    "    # .summary('count', 'mean', 'stddev', 'min', '25%', '50%', '75%', 'max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6PlMIPB4n9n"
   },
   "source": [
    "## Juntando DataFrames - Joins\n",
    "\n",
    "[DataFrame.join(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZtnUWTOp26lv"
   },
   "outputs": [],
   "source": [
    "produtos = spark.createDataFrame(\n",
    "    [\n",
    "        ('1', 'Bebidas', 'Água mineral'),\n",
    "        ('2', 'Limpeza', 'Sabão em pó'),\n",
    "        ('3', 'Frios', 'Queijo'),\n",
    "        ('4', 'Bebidas', 'Refrigerante'),\n",
    "        ('5', 'Pet', 'Ração para cães')\n",
    "    ],\n",
    "    ['id', 'cat', 'prod']\n",
    ")\n",
    "\n",
    "impostos = spark.createDataFrame(\n",
    "    [\n",
    "        ('Bebidas', 0.15),\n",
    "        ('Limpeza', 0.05),\n",
    "        ('Frios', 0.065),\n",
    "        ('Carnes', 0.08)\n",
    "    ],\n",
    "    ['cat', 'tax']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "er-73XyH6tp4",
    "outputId": "c6ec9ee5-bac9-4783-eacd-c7b190e86823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------------+-----+\n",
      "|    cat| id|        prod|  tax|\n",
      "+-------+---+------------+-----+\n",
      "|Bebidas|  1|Água mineral| 0.15|\n",
      "|Limpeza|  2| Sabão em pó| 0.05|\n",
      "|  Frios|  3|      Queijo|0.065|\n",
      "|Bebidas|  4|Refrigerante| 0.15|\n",
      "+-------+---+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produtos.join(impostos, 'cat', how='inner')\\\n",
    "    .sort('id')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFSbLhkd6uzL",
    "outputId": "0f2b2556-afd9-4f42-b220-51cfd2d322a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------------+-----+\n",
      "|    cat| id|           prod|  tax|\n",
      "+-------+---+---------------+-----+\n",
      "|Bebidas|  1|   Água mineral| 0.15|\n",
      "|Limpeza|  2|    Sabão em pó| 0.05|\n",
      "|  Frios|  3|         Queijo|0.065|\n",
      "|Bebidas|  4|   Refrigerante| 0.15|\n",
      "|    Pet|  5|Ração para cães| null|\n",
      "+-------+---+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produtos.join(impostos, 'cat', how='left')\\\n",
    "    .sort('id')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTpu8_-c6xQx",
    "outputId": "a481299e-b22d-4704-aba5-3d980dc3df56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+-----+\n",
      "|    cat|  id|        prod|  tax|\n",
      "+-------+----+------------+-----+\n",
      "| Carnes|null|        null| 0.08|\n",
      "|Bebidas|   1|Água mineral| 0.15|\n",
      "|Limpeza|   2| Sabão em pó| 0.05|\n",
      "|  Frios|   3|      Queijo|0.065|\n",
      "|Bebidas|   4|Refrigerante| 0.15|\n",
      "+-------+----+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produtos.join(impostos, 'cat', how='right')\\\n",
    "    .sort('id')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAm7w6vR6yec",
    "outputId": "ee7c3d28-9aab-4b2c-b35b-0fb2b2d304ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------------+-----+\n",
      "|    cat|  id|           prod|  tax|\n",
      "+-------+----+---------------+-----+\n",
      "| Carnes|null|           null| 0.08|\n",
      "|Bebidas|   1|   Água mineral| 0.15|\n",
      "|Limpeza|   2|    Sabão em pó| 0.05|\n",
      "|  Frios|   3|         Queijo|0.065|\n",
      "|Bebidas|   4|   Refrigerante| 0.15|\n",
      "|    Pet|   5|Ração para cães| null|\n",
      "+-------+----+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produtos.join(impostos, 'cat', how='outer')\\\n",
    "    .sort('id')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BN2OkSke6zsn"
   },
   "outputs": [],
   "source": [
    "empresas_join = estabelecimentos.join(empresas, 'cnpj_basico', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sM2K_HJf67lz"
   },
   "outputs": [],
   "source": [
    "freq = empresas_join\\\n",
    "    .select(\n",
    "        'cnpj_basico',\n",
    "        f.year('data_de_inicio_atividade').alias('data_de_inicio')\n",
    "    )\\\n",
    "    .where('data_de_inicio >= 2010')\\\n",
    "    .groupBy('data_de_inicio')\\\n",
    "    .agg(f.count(\"cnpj_basico\").alias(\"frequencia\"))\\\n",
    "    .orderBy('data_de_inicio', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1S3bEWNm7CSV",
    "outputId": "16a00143-f084-4db6-ae39-d6a472eadcda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|data_de_inicio|frequencia|\n",
      "+--------------+----------+\n",
      "|          2010|    154159|\n",
      "|          2011|    172677|\n",
      "|          2012|    232480|\n",
      "|          2013|    198424|\n",
      "|          2014|    202276|\n",
      "|          2015|    212523|\n",
      "|          2016|    265417|\n",
      "|          2017|    237292|\n",
      "|          2018|    275435|\n",
      "|          2019|    325922|\n",
      "|          2020|    400654|\n",
      "|          2021|    153275|\n",
      "|         Total|   2830534|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq.union(\n",
    "    freq.select(\n",
    "        f.lit('Total').alias('data_de_inicio'),\n",
    "        f.sum(freq.frequencia).alias('frequencia')\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrAxJ4Rk7G3r"
   },
   "source": [
    "## SparkSQL\n",
    "\n",
    "[SparkSession.sql(sqlQuery)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.sql.html)\n",
    "\n",
    "Para saber mais sobre performance: [Artigo - Spark RDDs vs DataFrames vs SparkSQL](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "w9Bb3UTd7IVU"
   },
   "outputs": [],
   "source": [
    "empresas.createOrReplaceTempView('empresasView')\n",
    "estabelecimentos.createOrReplaceTempView('estabelecimentosView')\n",
    "socios.createOrReplaceTempView('sociosView')\n",
    "empresas_join.createOrReplaceTempView('empresasJoinView')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJnDEwWM7rxu",
    "outputId": "220b0283-a70c-452b-8e23-ba327900654c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial|natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|\n",
      "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "|        306|         FRANCAMAR REFRIGE...|             2240|                         49|                      0.0|               1|                       null|\n",
      "|       1355|         BRASILEIRO & OLIV...|             2062|                         49|                      0.0|               5|                       null|\n",
      "|       4820|         REGISTRO DE IMOVE...|             3034|                         32|                      0.0|               5|                       null|\n",
      "|       5347|         ROSELY APARECIDA ...|             2135|                         50|                      0.0|               5|                       null|\n",
      "|       6846|         BADU E FILHOS TEC...|             2062|                         49|                   4000.0|               1|                       null|\n",
      "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM empresasView\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpT3cX6Y74dY",
    "outputId": "50e51512-0c0f-4b08-d363-ffa8a7ddc158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+-----------+----------+-------+---------------------------+--------------------+------------------+-----------------------+-------------------------+--------------------------+----+------------------------+---------------------+----------------------+------------------+--------------------+------+-----------+-------------+--------+---+---------+-----+----------+-----+----------+----------+--------+--------------------+-----------------+-------------------------+\n",
      "|cnpj_basico|razao_social_nome_empresarial|natureza_juridica|qualificacao_do_responsavel|capital_social_da_empresa|porte_da_empresa|ente_federativo_responsavel|cnpj_basico|cnpj_ordem|cnpj_dv|identificador_matriz_filial|       nome_fantasia|situacao_cadastral|data_situacao_cadastral|motivo_situacao_cadastral|nome_da_cidade_no_exterior|pais|data_de_inicio_atividade|cnae_fiscal_principal|cnae_fiscal_secundaria|tipo_de_logradouro|          logradouro|numero|complemento|       bairro|     cep| uf|municipio|ddd_1|telefone_1|ddd_2|telefone_2|ddd_do_fax|     fax|  correio_eletronico|situacao_especial|data_da_situacao_especial|\n",
      "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+-----------+----------+-------+---------------------------+--------------------+------------------+-----------------------+-------------------------+--------------------------+----+------------------------+---------------------+----------------------+------------------+--------------------+------+-----------+-------------+--------+---+---------+-----+----------+-----+----------+----------+--------+--------------------+-----------------+-------------------------+\n",
      "|      13289|         ANDREIA CRISTINA ...|             2305|                         65|                 100000.0|               1|                       null|      13289|         1|     83|                          1|JS MATERIAIS DE C...|                 2|             2004-01-23|                        0|                      null|null|              1994-06-01|              4744099|                  null|               RUA|  SALVADOR TAMAIOLLI|   165|       null|PORTO BELLO I|13660000| SP|     6915|   19|  35811286| null|      null|      null|    null|CONTATO@LEONECONT...|             null|                     null|\n",
      "|      28664|         M ROCHA COML IMPO...|             2062|                         49|                 100000.0|               5|                       null|      28664|         1|     69|                          1|                null|                 2|             2004-08-28|                        0|                      null|null|              1994-08-08|              4672900|               8211300|               RUA|               ROCHA|   173|       null| MEU CANTINHO| 8664650| SP|     7151|   19|  37134004| null|      null|      null|    null|CONTATO@CONCEITOS...|             null|                     null|\n",
      "|      65867|         PADADRIA POLITEAM...|             2062|                         49|                  30000.0|               1|                       null|      65867|         1|     25|                          1|                null|                 4|             2020-10-29|                       63|                      null|null|              1994-05-11|              4712100|       4721102,1091101|               RUA|  POLITEAMA DE BAIXO|    19|       null|    POLITEAMA|40080166| BA|     3849|   71|  33391500| null|      null|      null|    null|brasconte@brascon...|             null|                     null|\n",
      "|      90550|         A M VIEIRA MERCEA...|             2062|                         49|                 100000.0|               1|                       null|      90550|         1|     49|                          1| MERCADO TRES IRMAOS|                 4|             2018-10-24|                       63|                      null|null|              1994-06-13|              4712100|               4721102|           AVENIDA|JOAQUIM DA COSTA ...|  3952|       LOJA|  SANTA MARIA|26165345| RJ|     2909|   21|  26678952|   21|  78341139|        21|26682012|MAJILCONTABIL@YAH...|             null|                     null|\n",
      "|     100170|         ALCINO FILHO E OL...|             2062|                         49|                 140000.0|               1|                       null|     100170|         1|     48|                          1|CAPITAL FRIOS E S...|                 4|             2020-10-22|                       63|                      null|null|              1994-06-28|              4691500|  4639701,4721103,4...|               RUA|                   E|   S/N|  QUADRA 03|   DESPRAIADO|78048000| MT|     9067|   65|  84061982|   65|  36857255|        65|36857255|jproenca@hotmail.com|             null|                     null|\n",
      "+-----------+-----------------------------+-----------------+---------------------------+-------------------------+----------------+---------------------------+-----------+----------+-------+---------------------------+--------------------+------------------+-----------------------+-------------------------+--------------------------+----+------------------------+---------------------+----------------------+------------------+--------------------+------+-----------+-------------+--------+---+---------+-----+----------+-----+----------+----------+--------+--------------------+-----------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\\\n",
    "    .sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM empresasView e\n",
    "        JOIN estabelecimentosView est ON e.cnpj_basico = est.cnpj_basico\n",
    "        WHERE e.capital_social_da_empresa >= 10000\n",
    "        \"\"\"\n",
    "    )\\\n",
    "    .show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8AfoHEM8R4T",
    "outputId": "eaa4669a-35b1-4afd-b01e-22f66581e9f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|porte_da_empresa|media_capital_social|\n",
      "+----------------+--------------------+\n",
      "|            null|    8.35421888053467|\n",
      "|               1|  339994.53313507047|\n",
      "|               3|  2601001.7677092687|\n",
      "|               5|   708660.4208249793|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark\\\n",
    "    .sql(\n",
    "        \"\"\"\n",
    "        SELECT porte_da_empresa, MEAN(capital_social_da_empresa) AS media_capital_social\n",
    "        FROM empresasView e\n",
    "        GROUP BY porte_da_empresa\n",
    "        \"\"\"\n",
    "    )\\\n",
    "    .show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VML0yIJ9YU_",
    "outputId": "1ef05870-fd40-4a26-9c8a-3b11c1f15418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|data_de_inicio|frequencia|\n",
      "+--------------+----------+\n",
      "|          2010|    154159|\n",
      "|          2011|    172677|\n",
      "|          2012|    232480|\n",
      "|          2013|    198424|\n",
      "|          2014|    202276|\n",
      "|          2015|    212523|\n",
      "|          2016|    265415|\n",
      "|          2017|    237292|\n",
      "|          2018|    275435|\n",
      "|          2019|    325922|\n",
      "|          2020|    400654|\n",
      "|          2021|    153275|\n",
      "|         Total|   2830532|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq = spark\\\n",
    "    .sql(\n",
    "        \"\"\"\n",
    "          WITH freq AS (\n",
    "          SELECT YEAR(data_de_inicio_atividade) AS data_de_inicio, COUNT(cnpj_basico) AS frequencia\n",
    "          FROM estabelecimentosView\n",
    "          WHERE YEAR(data_de_inicio_atividade) >= 2010\n",
    "          GROUP BY data_de_inicio\n",
    "          ORDER BY data_de_inicio\n",
    "        )\n",
    "        SELECT * FROM freq\n",
    "        UNION ALL\n",
    "        SELECT 'Total', SUM(frequencia) FROM freq\n",
    "        \"\"\"\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc-fyKC__nzT"
   },
   "source": [
    "# Formas de Armazenamento\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nWleRq6_pz7"
   },
   "source": [
    "## Arquivos CSV\n",
    "\n",
    "[property DataFrame.write](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.write.html)\n",
    "\n",
    "[DataFrameWriter.csv(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "EGJh57GZ_ojq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocorreu um erro: An error occurred while calling o494.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 114.0 failed 1 times, most recent failure: Lost task 2.0 in stage 114.0 (TID 3247) (192.168.0.7 executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 H:\\Projetos\\PySpark-Data-Science-Intro\\data\\empresas\\resultado\\_temporary\\0\\_temporary\\attempt_202407210042544131845422780430293_0114_m_000002_3247\\part-00002-6922c132-a08a-4e42-9f9a-19a801f6810c-c000.csv\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\t... 32 more\n",
      "Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 H:\\Projetos\\PySpark-Data-Science-Intro\\data\\empresas\\resultado\\_temporary\\0\\_temporary\\attempt_202407210042544131845422780430293_0114_m_000002_3247\\part-00002-6922c132-a08a-4e42-9f9a-19a801f6810c-c000.csv\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para ambientes integrados com o hadoop, localmente não funcionará\n",
    "empresas.write.csv(\n",
    "    path=os.path.join(pathDefault, 'empresas', 'resultado'),\n",
    "    mode='overwrite',\n",
    "    header=True,\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "estabelecimentos.write.csv(\n",
    "    path=os.path.join(pathDefault, 'estabelecimentos', 'resultado'),\n",
    "    mode='overwrite',\n",
    "    header=True,\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "socios.write.csv(\n",
    "    path=os.path.join(pathDefault, 'socios', 'resultado'),\n",
    "    mode='overwrite',\n",
    "    header=True,\n",
    "    sep=';'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gv1IPDsBLxy"
   },
   "source": [
    "## Arquivos PARQUET\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/)\n",
    "\n",
    "[DataFrameWriter.parquet(*args)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.parquet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj2KayiXA1sq"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o452.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 111.0 failed 1 times, most recent failure: Lost task 7.0 in stage 111.0 (TID 3225) (192.168.0.7 executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 H:\\content\\drive\\MyDrive\\curso-spark\\spark-apresentacao-ferramenta\\empresas\\resultadoParquet\\_temporary\\0\\_temporary\\attempt_202407210030568438880548593270256_0111_m_000007_3225\\part-00007-6b459992-e942-4838-a8e0-0c32b26944d8-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 H:\\content\\drive\\MyDrive\\curso-spark\\spark-apresentacao-ferramenta\\empresas\\resultadoParquet\\_temporary\\0\\_temporary\\attempt_202407210030568438880548593270256_0111_m_000007_3225\\part-00007-6b459992-e942-4838-a8e0-0c32b26944d8-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Defina o modo de rebase de data e hora como 'LEGACY' para garantir compatibilidade com sistemas mais antigos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.legacy.parquet.datetimeRebaseModeInWrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEGACY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mempresas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/empresas/resultadoParquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m estabelecimentos\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\n\u001b[0;32m     10\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/estabelecimentos/resultadoParquet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m socios\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\n\u001b[0;32m     15\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/socios/resultadoParquet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py:1250\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o452.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 111.0 failed 1 times, most recent failure: Lost task 7.0 in stage 111.0 (TID 3225) (192.168.0.7 executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 H:\\content\\drive\\MyDrive\\curso-spark\\spark-apresentacao-ferramenta\\empresas\\resultadoParquet\\_temporary\\0\\_temporary\\attempt_202407210030568438880548593270256_0111_m_000007_3225\\part-00007-6b459992-e942-4838-a8e0-0c32b26944d8-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 H:\\content\\drive\\MyDrive\\curso-spark\\spark-apresentacao-ferramenta\\empresas\\resultadoParquet\\_temporary\\0\\_temporary\\attempt_202407210030568438880548593270256_0111_m_000007_3225\\part-00007-6b459992-e942-4838-a8e0-0c32b26944d8-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Para ambientes integrados com o hadoop, localmente não funcionará\n",
    "# Defina o modo de rebase de data e hora como 'LEGACY' para garantir compatibilidade com sistemas mais antigos\n",
    "spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")\n",
    "\n",
    "empresas.write.parquet(\n",
    "    path='/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/empresas/resultadoParquet',\n",
    "    mode='overwrite'\n",
    ")\n",
    "\n",
    "estabelecimentos.write.parquet(\n",
    "    path='/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/estabelecimentos/resultadoParquet',\n",
    "    mode='overwrite'\n",
    ")\n",
    "\n",
    "socios.write.parquet(\n",
    "    path='/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/socios/resultadoParquet',\n",
    "    mode='overwrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZfIIgkGGlLz"
   },
   "source": [
    "## Particionamento dos dados\n",
    "\n",
    "[DataFrameWriter.partitionBy(*cols)](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html)\n",
    "\n",
    "```\n",
    "# Isto está formatado como código\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-CKsbccBo0c"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o531.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mempresas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/empresas/resultado/csvUnico\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m estabelecimentos\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[0;32m      9\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/estabelecimentos/resultado/csvUnico\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m socios\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[0;32m     16\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/socios/resultado/csvUnico\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o531.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Para ambientes integrados com o hadoop, localmente não funcionará\n",
    "empresas.coalesce(1).write.csv(\n",
    "    path='/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/empresas/resultado/csvUnico',\n",
    "    mode='overwrite',\n",
    "    header=True,\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "estabelecimentos.coalesce(1).write.csv(\n",
    "    path='/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/estabelecimentos/resultado/csvUnico',\n",
    "    mode='overwrite',\n",
    "    header=True,\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "socios.coalesce(1).write.csv(\n",
    "    path='/content/drive/MyDrive/curso-spark/spark-apresentacao-ferramenta/socios/resultado/csvUnico',\n",
    "    mode='overwrite',\n",
    "    header=True,\n",
    "    sep=';'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ9L4dolIh7j"
   },
   "source": [
    "## Finalizando sessão spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12k8cvfSIkka"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHzAYcRYIpxE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
